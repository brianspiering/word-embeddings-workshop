{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Further Study\n",
    "------\n",
    "\n",
    "[Awesome NLP 4 DL](https://github.com/brianspiering/awesome-dl4nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Word Embeddings and friends\n",
    "----\n",
    "1. [The amazing power of word vectors](https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/) from The Morning Paper blog\n",
    "1. [Distributed Representations of Words and Phrases and their Compositionality](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) - The original word2vec paper.\n",
    "1. [word2vec Parameter Learning Explained](https://arxiv.org/abs/1411.2738) An elucidating explanation of word2vec training\n",
    "1. [Word embeddings in 2017: Trends and future directions](http://ruder.io/word-embeddings-2017/)\n",
    "1. [Learning Word Vectors for 157 Languages](https://arxiv.org/abs/1802.06893)\n",
    "1. [GloVe: Global Vectors for Word Representation](http://www-nlp.stanford.edu/pubs/glove.pdf) - A \"count-based\"/co-occurrence model to learn word embeddings.\n",
    "1.  Doc2Vec\n",
    "\t- [A gentle introduction to Doc2Vec](https://medium.com/scaleabout/a-gentle-introduction-to-doc2vec-db3e8c0cce5e)\n",
    "\t- [Distributed Representations of Sentences and Documents](https://cs.stanford.edu/~quocle/paragraph_vector.pdf)\n",
    "1. [Dynamic word embeddings for evolving semantic discovery](https://blog.acolyer.org/2018/02/22/dynamic-word-embeddings-for-evolving-semantic-discovery/) from The Morning Paper blog\n",
    "1. Ali Ghodsi's lecture on word2vec: \n",
    "\t- [part 1](https://www.youtube.com/watch?v=TsEGsdVJjuA)\n",
    "\t- [part 2](https://www.youtube.com/watch?v=nuirUEmbaJU)\n",
    "1. [word2vec analogy demo](http://deeplearner.fz-qqq.net/)\n",
    "1. [TensorFlow Embedding Projector of word vectors](http://projector.tensorflow.org/)\n",
    "1. Skip-Thought Vectors - \"unsupervised learning of a generic, distributed sentence encoder\"\n",
    "    - [Paper](http://arxiv.org/abs/1506.06726)\n",
    "    - [Code](https://github.com/ryankiros/skip-thoughts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "End-To-End Deep Learnning\n",
    "-------\n",
    "\n",
    "<center><img src=\"images/end.png\" width=\"700\"/></center>\n",
    "\n",
    "Word embeddings are learned along with task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "STOTA DL NLP architecture\n",
    "-------\n",
    "\n",
    "<center><img src=\"images/complete.png\" width=\"1000\"/></center>\n",
    "\n",
    "Elements: Word embeddings, Bi-Directional LSTM with attention, Meta-Learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Neural Networks Demystified Part 1-7:\n",
    "-----\n",
    "+ [Watch](https://www.youtube.com/watch?v=5MXp9UUkSmc) \n",
    "+ [Code](http://nbviewer.ipython.org/github/stephencwelch/Neural-Networks-Demysitifed/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br>\n",
    "<br> \n",
    "<br>\n",
    "\n",
    "----"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
